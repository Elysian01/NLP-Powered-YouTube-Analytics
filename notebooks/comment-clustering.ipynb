{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8250819,"sourceType":"datasetVersion","datasetId":4895585},{"sourceId":8254823,"sourceType":"datasetVersion","datasetId":4898608}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import re\ntry:\n    import contractions\nexcept ImportError:\n    !pip install contractions\n    import contractions\n\nfrom contractions import fix  # Ensure contractions library is installed\n\nimport string\nimport nltk\n# !pip install emoji","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:39.729626Z","iopub.execute_input":"2024-04-28T19:25:39.730029Z","iopub.status.idle":"2024-04-28T19:25:39.735790Z","shell.execute_reply.started":"2024-04-28T19:25:39.729997Z","shell.execute_reply":"2024-04-28T19:25:39.734752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading Dataset**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/yt-2k-comments/youtube_comments_2k.csv')  # data contains Validate Dataset\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:41.030509Z","iopub.execute_input":"2024-04-28T19:25:41.030903Z","iopub.status.idle":"2024-04-28T19:25:41.046486Z","shell.execute_reply.started":"2024-04-28T19:25:41.030873Z","shell.execute_reply":"2024-04-28T19:25:41.045605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:41.404404Z","iopub.execute_input":"2024-04-28T19:25:41.405363Z","iopub.status.idle":"2024-04-28T19:25:41.415643Z","shell.execute_reply.started":"2024-04-28T19:25:41.405325Z","shell.execute_reply":"2024-04-28T19:25:41.414488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:41.755201Z","iopub.execute_input":"2024-04-28T19:25:41.756097Z","iopub.status.idle":"2024-04-28T19:25:41.765206Z","shell.execute_reply.started":"2024-04-28T19:25:41.756061Z","shell.execute_reply":"2024-04-28T19:25:41.764199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"code","source":"df= pd.DataFrame(data) # df_mix frame contains comments of multiple languages\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:42.388816Z","iopub.execute_input":"2024-04-28T19:25:42.389216Z","iopub.status.idle":"2024-04-28T19:25:42.393818Z","shell.execute_reply.started":"2024-04-28T19:25:42.389187Z","shell.execute_reply":"2024-04-28T19:25:42.392875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:42.728667Z","iopub.execute_input":"2024-04-28T19:25:42.729550Z","iopub.status.idle":"2024-04-28T19:25:42.735189Z","shell.execute_reply.started":"2024-04-28T19:25:42.729513Z","shell.execute_reply":"2024-04-28T19:25:42.734272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()  # count the number of missing values (NaNs) in each column of a DataFrame df.\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:43.051468Z","iopub.execute_input":"2024-04-28T19:25:43.052131Z","iopub.status.idle":"2024-04-28T19:25:43.060015Z","shell.execute_reply.started":"2024-04-28T19:25:43.052099Z","shell.execute_reply":"2024-04-28T19:25:43.059035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if df.isnull().values.any():   # removes rows containing missing values (NaNs) from the DataFrame (As missing value present in df, we are removing here the respective rows)\n    df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:43.361363Z","iopub.execute_input":"2024-04-28T19:25:43.362316Z","iopub.status.idle":"2024-04-28T19:25:43.367758Z","shell.execute_reply.started":"2024-04-28T19:25:43.362272Z","shell.execute_reply":"2024-04-28T19:25:43.366793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape   # shape of df after removing missing values rows\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:43.779411Z","iopub.execute_input":"2024-04-28T19:25:43.780270Z","iopub.status.idle":"2024-04-28T19:25:43.785973Z","shell.execute_reply.started":"2024-04-28T19:25:43.780239Z","shell.execute_reply":"2024-04-28T19:25:43.784956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()  #used to count the number of unique values in each column of a DataFrame df.\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:43.974888Z","iopub.execute_input":"2024-04-28T19:25:43.975208Z","iopub.status.idle":"2024-04-28T19:25:43.985388Z","shell.execute_reply.started":"2024-04-28T19:25:43.975181Z","shell.execute_reply":"2024-04-28T19:25:43.984331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()  #used to get a concise summary of a DataFrame\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:44.224454Z","iopub.execute_input":"2024-04-28T19:25:44.224769Z","iopub.status.idle":"2024-04-28T19:25:44.235234Z","shell.execute_reply.started":"2024-04-28T19:25:44.224745Z","shell.execute_reply":"2024-04-28T19:25:44.234096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Standard NLP Techniques to Preprocess**","metadata":{}},{"cell_type":"markdown","source":"**Removing HTML Tags**","metadata":{}},{"cell_type":"code","source":"# Function to remove HTML tags\ndf2= pd.DataFrame(df)\ndef remove_html_tags(text):\n    pattern = re.compile('<.*?>')\n    return pattern.sub(r'', text)\n\ndf2['Comment'] = df2['Comment'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:45.247547Z","iopub.execute_input":"2024-04-28T19:25:45.247909Z","iopub.status.idle":"2024-04-28T19:25:45.257671Z","shell.execute_reply.started":"2024-04-28T19:25:45.247880Z","shell.execute_reply":"2024-04-28T19:25:45.256792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing URL's**","metadata":{}},{"cell_type":"code","source":"# Function to remove URLs\ndef remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'', text)\n\ndf2['Comment'] = df2['Comment'].apply(remove_url)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:46.780141Z","iopub.execute_input":"2024-04-28T19:25:46.780872Z","iopub.status.idle":"2024-04-28T19:25:46.798540Z","shell.execute_reply.started":"2024-04-28T19:25:46.780838Z","shell.execute_reply":"2024-04-28T19:25:46.797502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing New Lines**","metadata":{}},{"cell_type":"code","source":"# Function to remove newlines\ndef remove_newlines(text):\n    return text.replace('\\n', ' ')\n\ndf2['Comment'] = df2['Comment'].apply(remove_newlines)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:47.798389Z","iopub.execute_input":"2024-04-28T19:25:47.799234Z","iopub.status.idle":"2024-04-28T19:25:47.805878Z","shell.execute_reply.started":"2024-04-28T19:25:47.799201Z","shell.execute_reply":"2024-04-28T19:25:47.804721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Handling Emojis**","metadata":{}},{"cell_type":"code","source":"!pip install emoji","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:25:49.146470Z","iopub.execute_input":"2024-04-28T19:25:49.147173Z","iopub.status.idle":"2024-04-28T19:26:01.796190Z","shell.execute_reply.started":"2024-04-28T19:25:49.147141Z","shell.execute_reply":"2024-04-28T19:26:01.794984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import emoji\nimport re\ndef convert_emojis_to_text(text):\n  text = emoji.demojize(text).split(\":\")\n  text = \" \".join(text)\n  text = re.sub(r'\\s+', ' ', text)\n  return text\n\ndf2['Comment'] = df2['Comment'].apply(convert_emojis_to_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:26:01.798764Z","iopub.execute_input":"2024-04-28T19:26:01.799161Z","iopub.status.idle":"2024-04-28T19:26:02.519702Z","shell.execute_reply.started":"2024-04-28T19:26:01.799117Z","shell.execute_reply":"2024-04-28T19:26:02.518886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Handling Emoticons**","metadata":{}},{"cell_type":"code","source":"EMOTICONS = {\n    u\":‑\\)\": \"Happy face smiley\",\n    u\":\\)\": \"Happy face smiley\",\n    u\":-\\]\": \"Happy face smiley\",\n    u\":\\]\": \"Happy face smiley\",\n    u\":-3\": \"Happy face smiley\",\n    u\":3\": \"Happy face smiley\",\n    u\":->\": \"Happy face smiley\",\n    u\":>\": \"Happy face smiley\",\n    u\"8-\\)\": \"Happy face smiley\",\n    u\":o\\)\": \"Happy face smiley\",\n    u\":-\\}\": \"Happy face smiley\",\n    u\":\\}\": \"Happy face smiley\",\n    u\":-\\)\": \"Happy face smiley\",\n    u\":c\\)\": \"Happy face smiley\",\n    u\":\\^\\)\": \"Happy face smiley\",\n    u\"=\\]\": \"Happy face smiley\",\n    u\"=\\)\": \"Happy face smiley\",\n    u\":‑D\": \"Laughing, big grin or laugh with glasses\",\n    u\":D\": \"Laughing, big grin or laugh with glasses\",\n    u\"8‑D\": \"Laughing, big grin or laugh with glasses\",\n    u\"8D\": \"Laughing, big grin or laugh with glasses\",\n    u\"X‑D\": \"Laughing, big grin or laugh with glasses\",\n    u\"XD\": \"Laughing, big grin or laugh with glasses\",\n    u\"=D\": \"Laughing, big grin or laugh with glasses\",\n    u\"=3\": \"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\": \"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\": \"Very happy\",\n    u\":‑\\(\": \"Frown, sad, andry or pouting\",\n    u\":-\\(\": \"Frown, sad, andry or pouting\",\n    u\":\\(\": \"Frown, sad, andry or pouting\",\n    u\":‑c\": \"Frown, sad, andry or pouting\",\n    u\":c\": \"Frown, sad, andry or pouting\",\n    u\":‑<\": \"Frown, sad, andry or pouting\",\n    u\":<\": \"Frown, sad, andry or pouting\",\n    u\":‑\\[\": \"Frown, sad, andry or pouting\",\n    u\":\\[\": \"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\": \"Frown, sad, andry or pouting\",\n    u\">:\\[\": \"Frown, sad, andry or pouting\",\n    u\":\\{\": \"Frown, sad, andry or pouting\",\n    u\":@\": \"Frown, sad, andry or pouting\",\n    u\">:\\(\": \"Frown, sad, andry or pouting\",\n    u\":'‑\\(\": \"Crying\",\n    u\":'\\(\": \"Crying\",\n    u\":'‑\\)\": \"Tears of happiness\",\n    u\":'\\)\": \"Tears of happiness\",\n    u\"D‑':\": \"Horror\",\n    u\"D:<\": \"Disgust\",\n    u\"D:\": \"Sadness\",\n    u\"D8\": \"Great dismay\",\n    u\"D;\": \"Great dismay\",\n    u\"D=\": \"Great dismay\",\n    u\"DX\": \"Great dismay\",\n    u\":‑O\": \"Surprise\",\n    u\":O\": \"Surprise\",\n    u\":‑o\": \"Surprise\",\n    u\":o\": \"Surprise\",\n    u\":-0\": \"Shock\",\n    u\"8‑0\": \"Yawn\",\n    u\">:O\": \"Yawn\",\n    u\":-\\*\": \"Kiss\",\n    u\":\\*\": \"Kiss\",\n    u\":X\": \"Kiss\",\n    u\";‑\\)\": \"Wink or smirk\",\n    u\";\\)\": \"Wink or smirk\",\n    u\"\\*-\\)\": \"Wink or smirk\",\n    u\"\\*\\)\": \"Wink or smirk\",\n    u\";‑\\]\": \"Wink or smirk\",\n    u\";\\]\": \"Wink or smirk\",\n    u\";\\^\\)\": \"Wink or smirk\",\n    u\":‑,\": \"Wink or smirk\",\n    u\";D\": \"Wink or smirk\",\n    u\":‑P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X‑P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‑Þ\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":Þ\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‑/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":‑\\|\": \"Straight face\",\n    u\":\\|\": \"Straight face\",\n    u\":$\": \"Embarrassed or blushing\",\n    u\":‑x\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":‑#\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":‑&\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:‑\\)\": \"Angel, saint or innocent\",\n    u\"O:\\)\": \"Angel, saint or innocent\",\n    u\"0:‑3\": \"Angel, saint or innocent\",\n    u\"0:3\": \"Angel, saint or innocent\",\n    u\"0:‑\\)\": \"Angel, saint or innocent\",\n    u\"0:\\)\": \"Angel, saint or innocent\",\n    u\":‑b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\": \"Angel, saint or innocent\",\n    u\">:‑\\)\": \"Evil or devilish\",\n    u\">:\\)\": \"Evil or devilish\",\n    u\"\\}:‑\\)\": \"Evil or devilish\",\n    u\"\\}:\\)\": \"Evil or devilish\",\n    u\"3:‑\\)\": \"Evil or devilish\",\n    u\"3:\\)\": \"Evil or devilish\",\n    u\">;\\)\": \"Evil or devilish\",\n    u\"\\|;‑\\)\": \"Cool\",\n    u\"\\|‑O\": \"Bored\",\n    u\":‑J\": \"Tongue-in-cheek\",\n    u\"#‑\\)\": \"Party all night\",\n    u\"%‑\\)\": \"Drunk or confused\",\n    u\"%\\)\": \"Drunk or confused\",\n    u\":-###..\": \"Being sick\",\n    u\":###..\": \"Being sick\",\n    u\"<:‑\\|\": \"Dump\",\n    u\"\\(>_<\\)\": \"Troubled\",\n    u\"\\(>_<\\)>\": \"Troubled\",\n    u\"\\(';'\\)\": \"Baby\",\n    u\"\\(\\^\\^>``\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(・\\.・;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\": \"Sleeping\",\n    u\"\\(\\^_-\\)\": \"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\": \"Confused\",\n    u\"\\(\\+o\\+\\)\": \"Confused\",\n    u\"\\(o\\|o\\)\": \"Ultraman\",\n    u\"\\^_\\^\": \"Joyful\",\n    u\"\\(\\^_\\^\\)/\": \"Joyful\",\n    u\"\\(\\^O\\^\\)／\": \"Joyful\",\n    u\"\\(\\^o\\^\\)／\": \"Joyful\",\n    u\"\\(__\\)\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\": \"Sad or Crying\",\n    u\"\\(/_;\\)\": \"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\": \"Sad or Crying\",\n    u\"\\(;_;\": \"Sad of Crying\",\n    u\"\\(;_:\\)\": \"Sad or Crying\",\n    u\"\\(;O;\\)\": \"Sad or Crying\",\n    u\"\\(:_;\\)\": \"Sad or Crying\",\n    u\"\\(ToT\\)\": \"Sad or Crying\",\n    u\";_;\": \"Sad or Crying\",\n    u\";-;\": \"Sad or Crying\",\n    u\";n;\": \"Sad or Crying\",\n    u\";;\": \"Sad or Crying\",\n    u\"Q\\.Q\": \"Sad or Crying\",\n    u\"T\\.T\": \"Sad or Crying\",\n    u\"QQ\": \"Sad or Crying\",\n    u\"Q_Q\": \"Sad or Crying\",\n    u\"\\(-\\.-\\)\": \"Shame\",\n    u\"\\(-_-\\)\": \"Shame\",\n    u\"\\(一一\\)\": \"Shame\",\n    u\"\\(；一_一\\)\": \"Shame\",\n    u\"\\(=_=\\)\": \"Tired\",\n    u\"\\(=\\^\\·\\^=\\)\": \"cat\",\n    u\"\\(=\\^\\·\\·\\^=\\)\": \"cat\",\n    u\"=_\\^=\t\": \"cat\",\n    u\"\\(\\.\\.\\)\": \"Looking down\",\n    u\"\\(\\._\\.\\)\": \"Looking down\",\n    u\"\\^m\\^\": \"Giggling with hand covering mouth\",\n    u\"\\(\\・\\・?\": \"Confusion\",\n    u\"\\(?_?\\)\": \"Confusion\",\n    u\">\\^_\\^<\": \"Normal Laugh\",\n    u\"<\\^!\\^>\": \"Normal Laugh\",\n    u\"\\^/\\^\": \"Normal Laugh\",\n    u\"\\（\\*\\^_\\^\\*）\": \"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\": \"Normal Laugh\",\n    u\"\\(^\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\": \"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\": \"Normal Laugh\",\n    u\"\\(\\^—\\^\\）\": \"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\": \"Normal Laugh\",\n    u\"\\（\\^—\\^\\）\": \"Waving\",\n    u\"\\(;_;\\)/~~~\": \"Waving\",\n    u\"\\(\\^\\.\\^\\)/~~~\": \"Waving\",\n    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\": \"Waving\",\n    u\"\\(T_T\\)/~~~\": \"Waving\",\n    u\"\\(ToT\\)/~~~\": \"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\": \"Excited\",\n    u\"\\(\\*_\\*\\)\": \"Amazed\",\n    u\"\\(\\*_\\*;\": \"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\": \"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\": \"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\": \"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\": \"Headphones,Listening to music\",\n    u'\\(-\"-\\)': \"Worried\",\n    u\"\\(ーー;\\)\": \"Worried\",\n    u\"\\(\\^0_0\\^\\)\": \"Eyeglasses\",\n    u\"\\(\\＾ｖ\\＾\\)\": \"Happy\",\n    u\"\\(\\＾ｕ\\＾\\)\": \"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\": \"Happy\",\n    u\"\\(\\^O\\^\\)\": \"Happy\",\n    u\"\\(\\^o\\^\\)\": \"Happy\",\n    u\"\\)\\^o\\^\\(\": \"Happy\",\n    u\":O o_O\": \"Surprised\",\n    u\"o_0\": \"Surprised\",\n    u\"o\\.O\": \"Surpised\",\n    u\"\\(o\\.o\\)\": \"Surprised\",\n    u\"oO\": \"Surprised\",\n    u\"\\(\\*￣m￣\\)\": \"Dissatisfied\",\n    u\"\\(‘A`\\)\": \"Snubbed or Deflated\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:26:16.555876Z","iopub.execute_input":"2024-04-28T19:26:16.556279Z","iopub.status.idle":"2024-04-28T19:26:16.587168Z","shell.execute_reply.started":"2024-04-28T19:26:16.556251Z","shell.execute_reply":"2024-04-28T19:26:16.586212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_emoticons_to_text(text):\n    for emoticon, text_rep in EMOTICONS.items():\n        text = re.sub(emoticon, text_rep, text)\n    return text\n\ndf2['Comment'] = df2['Comment'].apply(convert_emoticons_to_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:26:18.748355Z","iopub.execute_input":"2024-04-28T19:26:18.748802Z","iopub.status.idle":"2024-04-28T19:26:19.381082Z","shell.execute_reply.started":"2024-04-28T19:26:18.748772Z","shell.execute_reply":"2024-04-28T19:26:19.380221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Text Lowercasing**\n","metadata":{}},{"cell_type":"code","source":"df2['Comment'] = df2['Comment'].str.lower()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:26:26.330908Z","iopub.execute_input":"2024-04-28T19:26:26.331749Z","iopub.status.idle":"2024-04-28T19:26:26.339743Z","shell.execute_reply.started":"2024-04-28T19:26:26.331711Z","shell.execute_reply":"2024-04-28T19:26:26.338453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Expanding Contractions**\n","metadata":{}},{"cell_type":"code","source":"# Function to expand contractions\ndef expand_contractions(text):\n    return fix(text)\ndf2['Comment'] = df2['Comment'].apply(expand_contractions)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:26:30.449848Z","iopub.execute_input":"2024-04-28T19:26:30.450463Z","iopub.status.idle":"2024-04-28T19:26:30.505170Z","shell.execute_reply.started":"2024-04-28T19:26:30.450420Z","shell.execute_reply":"2024-04-28T19:26:30.504314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:26:31.508288Z","iopub.execute_input":"2024-04-28T19:26:31.508955Z","iopub.status.idle":"2024-04-28T19:26:31.518474Z","shell.execute_reply.started":"2024-04-28T19:26:31.508922Z","shell.execute_reply":"2024-04-28T19:26:31.517469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing Punctuations**\n","metadata":{}},{"cell_type":"code","source":"# Function to remove punctuationdf2.head(15)\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\ndf2['Comment'] = df2['Comment'].apply(remove_punctuation)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:26:33.851657Z","iopub.execute_input":"2024-04-28T19:26:33.852040Z","iopub.status.idle":"2024-04-28T19:26:33.869987Z","shell.execute_reply.started":"2024-04-28T19:26:33.852010Z","shell.execute_reply":"2024-04-28T19:26:33.868999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df2.head(5))\nprint(df2.shape)\nprint(df2.info())","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:26:35.305640Z","iopub.execute_input":"2024-04-28T19:26:35.306528Z","iopub.status.idle":"2024-04-28T19:26:35.318983Z","shell.execute_reply.started":"2024-04-28T19:26:35.306492Z","shell.execute_reply":"2024-04-28T19:26:35.317952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Comment Clustering**","metadata":{}},{"cell_type":"markdown","source":"**1. Using TF-IDF**","metadata":{}},{"cell_type":"markdown","source":"**Using TF-IDF with predefined maximum cluster size(K-Means)**\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef find_optimal_cluster_size(df, max_clusters):\n    comments = df['Comment'].tolist()\n    tfidf_vectorizer = TfidfVectorizer()\n    tfidf_matrix = tfidf_vectorizer.fit_transform(comments)\n#     print(\"Shape of TF-IDF matrix:\", tfidf_matrix.shape)\n\n    inertias = []\n    silhouette_scores = []\n\n    for num_clusters in range(2, max_clusters + 1):\n        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n        kmeans.fit(tfidf_matrix)\n        clusters = kmeans.labels_\n        inertia = kmeans.inertia_\n        silhouette_avg = silhouette_score(tfidf_matrix, clusters)\n\n        inertias.append(inertia)\n        silhouette_scores.append(silhouette_avg)\n\n    # Plot Elbow Method\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(2, max_clusters + 1), inertias, marker='o')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Inertia')\n    plt.title('Elbow Method')\n\n    # Plot Silhouette Method\n    plt.subplot(1, 2, 2)\n    plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Silhouette Score')\n    plt.title('Silhouette Method')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Find the optimal cluster size based on the highest silhouette score\n    optimal_cluster_size = np.argmax(silhouette_scores) + 2\n    print(\"Optimal number of clusters based on Silhouette Method:\", optimal_cluster_size)\n    print(\"Silhouette Score TF-IDF(KMEANS Clustering):\", silhouette_scores[optimal_cluster_size - 2])\n\n    # Find the optimal cluster size based on the elbow method (just for comparison)\n    # Note: The Elbow Method is more subjective and may not always give a clear optimal point\n    diff = np.diff(inertias)\n    diff_r = diff[1:] / diff[:-1]\n    elbow_point = np.argmin(diff_r) + 1\n    print(\"Optimal number of clusters based on Elbow Method:\", elbow_point)\n    print(\"Inertia at Optimal Elbow Point:\", inertias[elbow_point - 2])\n\n    return optimal_cluster_size\n\n# Example usage\nmax_clusters = 20  # Maximum number of clusters to consider\noptimal_cluster_size = find_optimal_cluster_size(df2, max_clusters)\nprint(\"optimal_cluster_size for TF-IDF(KMEANS Clustering): \",optimal_cluster_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:12:27.716502Z","iopub.execute_input":"2024-04-28T20:12:27.717352Z","iopub.status.idle":"2024-04-28T20:12:41.828880Z","shell.execute_reply.started":"2024-04-28T20:12:27.717316Z","shell.execute_reply":"2024-04-28T20:12:41.827856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\n\ndef cluster_comments(df, num_clusters):\n    comments = df['Comment'].tolist()\n    \n    # Vectorize comments\n    tfidf_vectorizer = TfidfVectorizer()\n    tfidf_matrix = tfidf_vectorizer.fit_transform(comments)\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(tfidf_matrix)\n    clusters = kmeans.labels_\n    \n    # Calculate silhouette score to evaluate clustering quality\n    silhouette_avg = silhouette_score(tfidf_matrix, clusters)\n    print(f\"Silhouette Score TF-IDF(KMeans Clustering): {silhouette_avg}\")\n\n    # Assign cluster labels to DataFrame\n    df['Cluster'] = clusters\n\n    return df\n\n# Example usage\ndef analyze_optimal_cluster(df, optimal_cluster_size):\n    df = cluster_comments(df, optimal_cluster_size)\n    print(\"Cluster Analysis:\")\n    print(df.groupby('Cluster').size())\n\n# Example usage\n# You can replace this with the optimal cluster size found previously\nprint(\"Optimum_cluster_size: \",optimal_cluster_size)\nanalyze_optimal_cluster(df2, optimal_cluster_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:13:59.669063Z","iopub.execute_input":"2024-04-28T20:13:59.669929Z","iopub.status.idle":"2024-04-28T20:14:00.624410Z","shell.execute_reply.started":"2024-04-28T20:13:59.669895Z","shell.execute_reply":"2024-04-28T20:14:00.623449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing values in each cluster\n\ndef analyze_optimal_cluster(df, optimal_cluster_size):\n   \n    print(\"\\nComments in Each Cluster:\")\n    for cluster_id in range(optimal_cluster_size):\n        cluster_df = df[df['Cluster'] == cluster_id]\n        print(f\"\\nCluster {cluster_id}:\")\n        for comment in cluster_df['Comment']:\n            print(\">>\",comment)\n\n# Example usage\nanalyze_optimal_cluster(df2, optimal_cluster_size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using TF-IDF Without Predefined Cluster Size(DBSCAN clustering)**\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\ndef cluster_comments(df, eps=0.5, min_samples=5):\n    # Extract comments from DataFrame\n    comments = df['Comment'].tolist()\n    \n    # Vectorize comments\n    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.2, stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(comments)\n#     print(\"Shape of TF-IDF matrix:\", tfidf_matrix.shape)\n\n    # Apply DBSCAN clustering\n    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n    clusters = dbscan.fit_predict(tfidf_matrix)\n    \n    # Calculate silhouette score to evaluate clustering quality\n    silhouette_avg = silhouette_score(tfidf_matrix, clusters)\n    print(f\"Silhouette Score for TF-IDF(DBSCAN Clustering): {silhouette_avg}\")\n    \n    return clusters\n\n# Example usage\ncomment_clusters = cluster_comments(df2)\n\n# print(\"Comment Clusters: \",comment_clusters)\nunique_labels = np.unique(comment_clusters)\nprint(\"Distinct Comment Cluster Labels By TF-IDF(DBSCAN Clustering): \", unique_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:07:56.595334Z","iopub.execute_input":"2024-04-28T20:07:56.596304Z","iopub.status.idle":"2024-04-28T20:07:56.799112Z","shell.execute_reply.started":"2024-04-28T20:07:56.596264Z","shell.execute_reply":"2024-04-28T20:07:56.797922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_comments_in_clusters(df, clusters):\n    # Iterate over unique cluster labels\n    for cluster_label in np.unique(clusters):\n        # If cluster_label is -1, it represents noise points\n        if cluster_label == -1:\n            print(\"Noise Points:\")\n            noise_indices = np.where(clusters == cluster_label)[0]\n            for index in noise_indices:\n                print(df['Comment'][index])\n        else:\n            print(f\"Cluster {cluster_label}:\")\n            cluster_indices = np.where(clusters == cluster_label)[0]\n            for index in cluster_indices:\n                print(\">> \",df['Comment'][index])\n        print(\"\\n\")\n\n# Example usage\nprint_comments_in_clusters(df2, comment_clusters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2**. **GloVe's Embeddings.**","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\nimport gensim.downloader as api\n\ndef download_glove_embeddings(save_path):\n    # Download GloVe embeddings from gensim's API\n    glove_vectors = api.load(\"glove-wiki-gigaword-300\")\n    # Save the embeddings to a file\n    glove_vectors.save_word2vec_format(save_path, binary=True)\n\ndef load_word_embeddings(embeddings_file):\n    # Load pre-trained word embeddings (GloVe format)\n    word_vectors = api.load(\"glove-wiki-gigaword-300\")\n    return word_vectors\n\ndef comment_to_embedding(comment, word_vectors):\n    # Convert a comment to its embedding representation\n    words = comment.split()\n    # Filter words that are present in the vocabulary\n    words = [word for word in words if word in word_vectors]\n    if len(words) == 0:\n        return None\n    # Get word embeddings for the words in the comment\n    comment_embeddings = [word_vectors[word] for word in words]\n    return np.mean(comment_embeddings, axis=0)\n\ndef cluster_comments_with_word_embeddings(comments, word_vectors, num_clusters):\n    # Convert comments to embeddings\n    comment_embeddings = [comment_to_embedding(comment, word_vectors) for comment in comments]\n    # Filter out comments with None embeddings\n    comment_embeddings = [embedding for embedding in comment_embeddings if embedding is not None]\n    # Convert embeddings to array\n    X = np.array(comment_embeddings)\n    \n    # Reduce dimensionality using PCA\n    pca = PCA(n_components=50)  # You can adjust the number of components as needed\n    X_pca = pca.fit_transform(X)\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(X_pca)\n    clusters = kmeans.labels_\n    \n    # Calculate silhouette score to evaluate clustering quality\n    silhouette_avg = silhouette_score(X_pca, clusters)\n    print(f\"Silhouette Score for GloVe's Word2Vec: {silhouette_avg}\")\n    return clusters\n\n# Example usage\n# Download GloVe embeddings\nsave_path = \"/kaggle/working/glove-wiki-gigaword-300.bin\"\ndownload_glove_embeddings(save_path)\n\n# Load pre-trained GloVe embeddings\nword_vectors = load_word_embeddings(save_path)\n\n# Cluster comments\nnum_clusters = 4  # Number of clusters\ncomment_clusters = cluster_comments_with_word_embeddings(df2['Comment'], word_vectors, num_clusters)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T19:57:24.118962Z","iopub.execute_input":"2024-04-28T19:57:24.119677Z","iopub.status.idle":"2024-04-28T20:01:21.746478Z","shell.execute_reply.started":"2024-04-28T19:57:24.119630Z","shell.execute_reply":"2024-04-28T20:01:21.744974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_labels = np.unique(comment_clusters)\nprint(\"Distinct Comment Cluster Labels By Glove's Word2Vec: \",unique_labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:06:35.208271Z","iopub.execute_input":"2024-04-28T20:06:35.208673Z","iopub.status.idle":"2024-04-28T20:06:35.214186Z","shell.execute_reply.started":"2024-04-28T20:06:35.208641Z","shell.execute_reply":"2024-04-28T20:06:35.213322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.** **BERT For Word Embedding**\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:16:19.461603Z","iopub.execute_input":"2024-04-28T20:16:19.462413Z","iopub.status.idle":"2024-04-28T20:16:19.467175Z","shell.execute_reply.started":"2024-04-28T20:16:19.462379Z","shell.execute_reply":"2024-04-28T20:16:19.466018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:16:22.812377Z","iopub.execute_input":"2024-04-28T20:16:22.812781Z","iopub.status.idle":"2024-04-28T20:16:22.818300Z","shell.execute_reply.started":"2024-04-28T20:16:22.812750Z","shell.execute_reply":"2024-04-28T20:16:22.817289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load pretrained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\nmodel.to(device)  # Move model to GPU if available\n\ndef tokenize_and_embed_comments(comments, tokenizer, model, device):\n    word_embeddings = []\n    for comment in comments:\n        # Truncate or chunk the comment to fit within the maximum sequence length\n        if len(comment) > tokenizer.model_max_length:\n            comment = comment[:tokenizer.model_max_length]  # Truncate to maximum length\n        # Tokenize the comment\n        tokens = tokenizer.encode(comment, add_special_tokens=False)\n        # Convert token IDs to tensor and move to GPU\n        tokens_tensor = torch.tensor([tokens]).to(device)\n        # Obtain BERT embeddings for the tokens\n        with torch.no_grad():\n            outputs = model(tokens_tensor)\n            embeddings = outputs.last_hidden_state.squeeze(0)  # Take embeddings from the last hidden layer\n        # Average the token embeddings to get the comment embedding\n        comment_embedding = torch.mean(embeddings, dim=0).cpu().numpy()  # Move back to CPU\n        word_embeddings.append(comment_embedding)\n    return np.array(word_embeddings)\n\n\ndef cluster_comments_with_bert_embeddings(comments, num_clusters):\n    # Tokenize and obtain BERT embeddings for the comments\n    word_embeddings = tokenize_and_embed_comments(comments, tokenizer, model, device)\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(word_embeddings)\n    clusters = kmeans.labels_\n    # Calculate silhouette score to evaluate clustering quality\n    silhouette_avg = silhouette_score(word_embeddings, clusters)\n    print(f\"Silhouette Score for BERT Word Embeddings: {silhouette_avg}\")\n    return clusters\n\n# Example usage\ncomments = df2['Comment'].tolist()\nnum_clusters = 7  # Number of clusters\ncomment_clusters = cluster_comments_with_bert_embeddings(comments, num_clusters)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:36:34.900883Z","iopub.execute_input":"2024-04-28T20:36:34.901302Z","iopub.status.idle":"2024-04-28T20:37:01.249375Z","shell.execute_reply.started":"2024-04-28T20:36:34.901271Z","shell.execute_reply":"2024-04-28T20:37:01.248249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pretrained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\nmodel.to(device)  # Move model to GPU if available\n\ndef generate_sentence_embedding(sentence, tokenizer, model, device, max_length=512):\n    # Tokenize the sentence\n    tokens = tokenizer.encode(sentence, add_special_tokens=True, max_length=max_length, truncation=True)\n    # Convert token IDs to tensor and move to GPU\n    tokens_tensor = torch.tensor([tokens]).to(device)\n    # Obtain BERT embeddings for the tokens\n    with torch.no_grad():\n        outputs = model(tokens_tensor)\n        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token embedding\n    # Move back to CPU and convert to numpy array\n    cls_embedding = cls_embedding.cpu().numpy()\n    return cls_embedding\n\ndef cluster_comments_with_bert_sentence_embeddings(comments, num_clusters):\n    sentence_embeddings = []\n    for comment in comments:\n        # Generate BERT embedding for each comment\n        embedding = generate_sentence_embedding(comment, tokenizer, model, device)\n        sentence_embeddings.append(embedding)\n    # Convert embeddings to numpy array\n    sentence_embeddings = np.array(sentence_embeddings)\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(sentence_embeddings.reshape(len(comments), -1))  # Reshape for clustering\n    clusters = kmeans.labels_\n    # Calculate silhouette score to evaluate clustering quality\n    silhouette_avg = silhouette_score(sentence_embeddings.reshape(len(comments), -1), clusters)\n    print(f\"Silhouette Score for BERT Sentense Embeddings: {silhouette_avg}\")\n    return clusters\n\n# Example usage\ncomments = df2['Comment'].tolist()\nnum_clusters = 4  # Number of clusters\ncomment_clusters = cluster_comments_with_bert_sentence_embeddings(comments, num_clusters)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:19:07.966340Z","iopub.execute_input":"2024-04-28T20:19:07.966748Z","iopub.status.idle":"2024-04-28T20:19:34.420006Z","shell.execute_reply.started":"2024-04-28T20:19:07.966708Z","shell.execute_reply":"2024-04-28T20:19:34.418638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. BigBird**","metadata":{}},{"cell_type":"code","source":"# Import BigBird model and tokenizer\nfrom transformers import BigBirdModel\n\n# Load pretrained BigBird model and tokenizer\nmodel = BigBirdModel.from_pretrained(\"google/bigbird-roberta-large\",attention_type=\"original_full\")\n# tokenizer = BigBirdTokenizer.from_pretrained(model)\n# model = BigBirdForSequenceClassification.from_pretrained(model)\nmodel.to(device)  # Move model to GPU if available\n\n# Update generate_sentence_embedding function to use BigBird\ndef generate_sentence_embedding(sentence, tokenizer, model, device):\n    # Tokenize the sentence\n    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs.to(device)\n    # Obtain BigBird embeddings for the tokens\n    with torch.no_grad():\n        outputs = model(**inputs)\n        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token embedding\n    # Move back to CPU and convert to numpy array\n    cls_embedding = cls_embedding.cpu().numpy()\n    return cls_embedding\n\n# The rest of your code remains the same\ndef cluster_comments_with_bigbird_sentence_embeddings(comments, num_clusters):\n    sentence_embeddings = []\n    for comment in comments:\n        # Generate BERT embedding for each comment\n        embedding = generate_sentence_embedding(comment, tokenizer, model, device)\n        sentence_embeddings.append(embedding)\n    # Convert embeddings to numpy array\n    sentence_embeddings = np.array(sentence_embeddings)\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(sentence_embeddings.reshape(len(comments), -1))  # Reshape for clustering\n    clusters = kmeans.labels_\n    # Calculate silhouette score to evaluate clustering quality\n    silhouette_avg = silhouette_score(sentence_embeddings.reshape(len(comments), -1), clusters)\n    print(f\"Silhouette Score for BigBird: {silhouette_avg}\")\n    return clusters\n\n# Example usage\ncomments = df2['Comment'].tolist()\nnum_clusters = 4  # Number of clusters\ncomment_clusters = cluster_comments_with_bigbird_sentence_embeddings(comments, num_clusters)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:30:54.944999Z","iopub.execute_input":"2024-04-28T20:30:54.945394Z","iopub.status.idle":"2024-04-28T20:31:49.834901Z","shell.execute_reply.started":"2024-04-28T20:30:54.945362Z","shell.execute_reply":"2024-04-28T20:31:49.833573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Assuming 'device' is defined earlier as the GPU device, e.g., device = torch.device(\"cuda\")\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.to(device)\nmodel.eval()\n\ndef encode_comments_bert(df):\n    # Encode comments using BERT embeddings\n\n    # Extract comments from DataFrame\n    if 'Comment' not in df.columns:\n        raise ValueError(\"DataFrame does not contain 'Comment' column.\")\n    comments = df['Comment'].tolist()\n\n    encoded_comments = []\n    for comment in comments:\n        inputs = tokenizer(comment, return_tensors='pt', padding=True, truncation=True)\n        inputs = {key: value.to(device) for key, value in inputs.items()}  # Move input tensors to GPU\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n        encoded_comment = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()  # Move output tensor back to CPU\n\n        encoded_comments.append(encoded_comment)\n    return np.concatenate(encoded_comments, axis=0)\n\ndef cluster_comments_bert(df, num_clusters):\n    # Encode comments using BERT embeddings\n    bert_embeddings = encode_comments_bert(df)\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n    kmeans.fit(bert_embeddings)\n    clusters = kmeans.labels_\n\n    # Calculate silhouette score to evaluate clustering quality\n    silhouette_avg = silhouette_score(bert_embeddings, clusters)\n    print(f\"Silhouette Score: {silhouette_avg}\")\n\n    return clusters\n\n# Example usage\nnum_clusters = 7  # Number of clusters\ncomment_clusters = cluster_comments_bert(df2, num_clusters)\nprint(np.unique(comment_clusters))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:37:34.742171Z","iopub.execute_input":"2024-04-28T20:37:34.742899Z","iopub.status.idle":"2024-04-28T20:38:01.267983Z","shell.execute_reply.started":"2024-04-28T20:37:34.742866Z","shell.execute_reply":"2024-04-28T20:38:01.266851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Using Topic Modelling**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\ndef cluster_comments_lda(comments, num_topics):\n    # Preprocess comments\n    vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n    X = vectorizer.fit_transform(comments)\n\n    # Apply LDA\n    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n    lda.fit(X)\n\n    # Get topic distributions for comments\n    topic_distributions = lda.transform(X)\n\n    # Assign each comment to the topic with the highest probability\n    comment_clusters = np.argmax(topic_distributions, axis=1)\n\n    return comment_clusters\n\n# Example usage\ncomments = df2['Comment'].tolist()\nnum_topics = 4  # Number of topics\ncomment_clusters = cluster_comments_lda(comments, num_topics)\nunique_labels = np.unique(comment_clusters)\nprint(\"Distinct Labels Using Topic Modelling: \", unique_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:43:56.597626Z","iopub.execute_input":"2024-04-28T20:43:56.598355Z","iopub.status.idle":"2024-04-28T20:44:01.806008Z","shell.execute_reply.started":"2024-04-28T20:43:56.598320Z","shell.execute_reply":"2024-04-28T20:44:01.804917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef calculate_silhouette_score(comments, comment_clusters):\n    # Convert comments to topic distributions\n    vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n    X = vectorizer.fit_transform(comments)\n\n    # Calculate pairwise cosine similarity between topic distributions of comments\n    topic_distributions = LatentDirichletAllocation(n_components=num_topics, random_state=42).fit_transform(X)\n    pairwise_similarity = cosine_similarity(topic_distributions)\n\n    # Calculate Silhouette score\n    silhouette_avg = silhouette_score(pairwise_similarity, comment_clusters)\n    print(f\"Silhouette Score for Topic Modelling based Clustering: {silhouette_avg}\")\n\n    return silhouette_avg\n\n# Example usage\nsilhouette_avg = calculate_silhouette_score(comments, comment_clusters)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:44:52.781705Z","iopub.execute_input":"2024-04-28T20:44:52.782091Z","iopub.status.idle":"2024-04-28T20:44:58.242996Z","shell.execute_reply.started":"2024-04-28T20:44:52.782060Z","shell.execute_reply":"2024-04-28T20:44:58.241657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_comments_in_clusters(df, clusters):\n    # Iterate over unique cluster labels\n    for cluster_label in np.unique(clusters):\n        # If cluster_label is -1, it represents noise points\n        if cluster_label == -1:\n            print(\"Noise Points:\")\n            noise_indices = np.where(clusters == cluster_label)[0]\n            for index in noise_indices:\n                print(df['Comment'][index])\n        else:\n            print(f\"Cluster {cluster_label}:\")\n            cluster_indices = np.where(clusters == cluster_label)[0]\n            for index in cluster_indices:\n                print(\">> \",df['Comment'][index])\n        print(\"\\n\")\n\n# Example usage\nprint_comments_in_clusters(df2, comment_clusters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Using BERTopic**","metadata":{}},{"cell_type":"code","source":"pip install bertopic\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bertopic import BERTopic\nfrom sentence_transformers import SentenceTransformer\n\n# Load comments from DataFrame\ncomments = df2['Comment'].astype(str).tolist()  # Ensure comments are converted to strings\n\n# Example function to generate BERT embeddings for comments\ndef generate_bert_embeddings(comments):\n    # Use SentenceTransformer to generate BERT embeddings for the comments\n    model = SentenceTransformer('bert-base-nli-mean-tokens')\n    model.to(device)  # Move model to GPU if available\n    embeddings = model.encode(comments)\n    return np.array(embeddings)\n\n# Generate BERT embeddings for comments\ncomment_embeddings = generate_bert_embeddings(comments)\n\n# Apply BERTopic for clustering\nbertopic_model = BERTopic()\ntopics, probs = bertopic_model.fit_transform(comments)  # Pass comments as strings\n\n# Get the cluster labels for each comment\ncluster_labels = topics\n\nunique_labels = np.unique(cluster_labels)\nprint(\"Distinct Labels: \", unique_labels)\n\n# Calculate Silhouette score\nsilhouette_avg = silhouette_score(comment_embeddings, cluster_labels)\nprint(f\"Silhouette Score for BERTopic: {silhouette_avg}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T20:48:05.388217Z","iopub.execute_input":"2024-04-28T20:48:05.388973Z","iopub.status.idle":"2024-04-28T20:48:22.871510Z","shell.execute_reply.started":"2024-04-28T20:48:05.388933Z","shell.execute_reply":"2024-04-28T20:48:22.870486Z"},"trusted":true},"execution_count":null,"outputs":[]}]}