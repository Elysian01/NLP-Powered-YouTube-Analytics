{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8124902,"sourceType":"datasetVersion","datasetId":4801495}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-15T12:33:14.560679Z","iopub.execute_input":"2024-04-15T12:33:14.561050Z","iopub.status.idle":"2024-04-15T12:33:15.606935Z","shell.execute_reply.started":"2024-04-15T12:33:14.561019Z","shell.execute_reply":"2024-04-15T12:33:15.605974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import re\nimport string\nimport spacy\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\ntry:\n    import contractions\nexcept ImportError:\n    !pip install contractions\n    import contractions\n\nfrom contractions import fix  # Ensure contractions library is installed\nfrom contractions import fix  # Assuming you have a function to expand contractions\n# Load English language model for lemmatization\nnlp = spacy.load(\"en_core_web_sm\")\n!pip install rouge\n!pip install bert_score ","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:33:18.084120Z","iopub.execute_input":"2024-04-15T12:33:18.084722Z","iopub.status.idle":"2024-04-15T12:34:08.125271Z","shell.execute_reply.started":"2024-04-15T12:33:18.084682Z","shell.execute_reply":"2024-04-15T12:34:08.124056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading Dataset**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/100row/YT_Summary_100_ROW.csv')  # data contains Validate Dataset\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:35:22.013160Z","iopub.execute_input":"2024-04-15T12:35:22.013545Z","iopub.status.idle":"2024-04-15T12:35:22.041786Z","shell.execute_reply.started":"2024-04-15T12:35:22.013514Z","shell.execute_reply":"2024-04-15T12:35:22.040735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:35:24.823879Z","iopub.execute_input":"2024-04-15T12:35:24.824680Z","iopub.status.idle":"2024-04-15T12:35:24.828920Z","shell.execute_reply.started":"2024-04-15T12:35:24.824648Z","shell.execute_reply":"2024-04-15T12:35:24.827929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:35:27.409460Z","iopub.execute_input":"2024-04-15T12:35:27.410268Z","iopub.status.idle":"2024-04-15T12:35:27.416627Z","shell.execute_reply.started":"2024-04-15T12:35:27.410213Z","shell.execute_reply":"2024-04-15T12:35:27.415501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:35:30.444482Z","iopub.execute_input":"2024-04-15T12:35:30.444838Z","iopub.status.idle":"2024-04-15T12:35:30.472232Z","shell.execute_reply.started":"2024-04-15T12:35:30.444809Z","shell.execute_reply":"2024-04-15T12:35:30.471237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"code","source":"df=pd.DataFrame()\ndf=data\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:35:51.047256Z","iopub.execute_input":"2024-04-15T12:35:51.047633Z","iopub.status.idle":"2024-04-15T12:35:51.055098Z","shell.execute_reply.started":"2024-04-15T12:35:51.047602Z","shell.execute_reply":"2024-04-15T12:35:51.054130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()  # count the number of missing values (NaNs) in each column of a DataFrame df.","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:36:08.701902Z","iopub.execute_input":"2024-04-15T12:36:08.702306Z","iopub.status.idle":"2024-04-15T12:36:08.710261Z","shell.execute_reply.started":"2024-04-15T12:36:08.702274Z","shell.execute_reply":"2024-04-15T12:36:08.709317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if df.isnull().values.any():   # removes rows containing missing values (NaNs) from the DataFrame (As missing value present in df, we are removing here the respective rows)\n    df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:36:18.217913Z","iopub.execute_input":"2024-04-15T12:36:18.218283Z","iopub.status.idle":"2024-04-15T12:36:18.226346Z","shell.execute_reply.started":"2024-04-15T12:36:18.218252Z","shell.execute_reply":"2024-04-15T12:36:18.225280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()  #used to count the number of unique values in each column of a DataFrame df.","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:36:25.449877Z","iopub.execute_input":"2024-04-15T12:36:25.450241Z","iopub.status.idle":"2024-04-15T12:36:25.460774Z","shell.execute_reply.started":"2024-04-15T12:36:25.450194Z","shell.execute_reply":"2024-04-15T12:36:25.459908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()  #used to get a concise summary of a DataFrame","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:37:20.183316Z","iopub.execute_input":"2024-04-15T12:37:20.183716Z","iopub.status.idle":"2024-04-15T12:37:20.194105Z","shell.execute_reply.started":"2024-04-15T12:37:20.183684Z","shell.execute_reply":"2024-04-15T12:37:20.193062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:37:30.244058Z","iopub.execute_input":"2024-04-15T12:37:30.244890Z","iopub.status.idle":"2024-04-15T12:37:30.250604Z","shell.execute_reply.started":"2024-04-15T12:37:30.244856Z","shell.execute_reply":"2024-04-15T12:37:30.249710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming df is your original DataFrame containing the \"chunks\" column\n# Create a new DataFrame df2 with only the \"chunks\" column\ndf2 = pd.DataFrame()\ndf2['chunks'] = df['chunks']\ndf2.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:37:47.296031Z","iopub.execute_input":"2024-04-15T12:37:47.296413Z","iopub.status.idle":"2024-04-15T12:37:47.307468Z","shell.execute_reply.started":"2024-04-15T12:37:47.296384Z","shell.execute_reply":"2024-04-15T12:37:47.306442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:37:59.249880Z","iopub.execute_input":"2024-04-15T12:37:59.250341Z","iopub.status.idle":"2024-04-15T12:37:59.258979Z","shell.execute_reply.started":"2024-04-15T12:37:59.250304Z","shell.execute_reply":"2024-04-15T12:37:59.258022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:38:05.369953Z","iopub.execute_input":"2024-04-15T12:38:05.370637Z","iopub.status.idle":"2024-04-15T12:38:05.380027Z","shell.execute_reply.started":"2024-04-15T12:38:05.370602Z","shell.execute_reply":"2024-04-15T12:38:05.379067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Parsing**","metadata":{}},{"cell_type":"markdown","source":"## **1. Shallow Parsing**","metadata":{}},{"cell_type":"markdown","source":"### **1.1 POS Tagging**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\ndef shallow_parsing(chunk):\n    if not isinstance(chunk, str):\n        # Handle non-string chunks (if needed)\n        return []\n    # Tokenize the chunk into sentences\n    sentences = sent_tokenize(chunk)\n    pos_tagged_sentences = []\n    for sentence in sentences:\n        # Tokenize each sentence into words\n        tokens = word_tokenize(sentence)\n        # Perform POS tagging on the list of tokens\n        pos_tags = nltk.pos_tag(tokens)\n        pos_tagged_sentences.append(pos_tags)\n    return pos_tagged_sentences\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:42:41.277635Z","iopub.execute_input":"2024-04-15T12:42:41.278024Z","iopub.status.idle":"2024-04-15T12:42:41.306002Z","shell.execute_reply.started":"2024-04-15T12:42:41.277994Z","shell.execute_reply":"2024-04-15T12:42:41.305114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2['pos_tagged_chunks']= df2['chunks'].apply(shallow_parsing)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:42:48.850338Z","iopub.execute_input":"2024-04-15T12:42:48.850724Z","iopub.status.idle":"2024-04-15T12:42:51.463689Z","shell.execute_reply.started":"2024-04-15T12:42:48.850680Z","shell.execute_reply":"2024-04-15T12:42:51.462677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:43:04.096869Z","iopub.execute_input":"2024-04-15T12:43:04.097618Z","iopub.status.idle":"2024-04-15T12:43:04.815432Z","shell.execute_reply.started":"2024-04-15T12:43:04.097568Z","shell.execute_reply":"2024-04-15T12:43:04.814485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Deep Parsing**","metadata":{}},{"cell_type":"markdown","source":"### **Chunk Dependency Visualization**","metadata":{}},{"cell_type":"markdown","source":"Here, we showcase dependency parsing using SpaCy on a selected chunk from DataFrame df2. Dependency parsing unveils grammatical relationships between words, visualized through displaCy's arrows, offering insight into the text's structural organization.","metadata":{}},{"cell_type":"code","source":"from spacy import displacy\n\n# Load the English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Assuming df2 is your DataFrame with a column named 'chunks'\n# Get the content of the first row in the 'chunks' column\nsentence = df2['chunks'].iloc[0]\n\n# Process the sentence using SpaCy\ndoc = nlp(sentence)\n\n# Visualize the dependency parse tree\ndisplacy.render(doc, style=\"dep\", jupyter=True, options={'compact': True})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:45:56.197864Z","iopub.execute_input":"2024-04-15T12:45:56.198587Z","iopub.status.idle":"2024-04-15T12:45:57.611204Z","shell.execute_reply.started":"2024-04-15T12:45:56.198551Z","shell.execute_reply":"2024-04-15T12:45:57.610236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Dependency and POS Tagging - Based Summarization of Chunked Text**","metadata":{}},{"cell_type":"code","source":"# Function to extract key phrases using dependency parsing\ndef extract_key_phrases(chunk):\n    doc = nlp(chunk)\n    key_phrases = []\n    for token in doc:\n        # Consider tokens that are nouns, verbs, adjectives, or adverbs\n        if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n            # Add token text to key phrases\n            key_phrases.append(token.text)\n        # Also consider tokens that are dependent on nouns, verbs, adjectives, or adverbs\n        if token.dep_ in [\"nsubj\", \"dobj\", \"attr\", \"advmod\", \"acomp\"]:\n            # Add dependent token text to key phrases\n            key_phrases.append(token.text)\n    return key_phrases\n\n# Function to generate summary using dependency parsing\ndef generate_dependency_summary_batch(texts):\n    summaries = []\n    for text in texts:\n        # Extract key phrases using dependency parsing\n        key_phrases = extract_key_phrases(text)\n        # Combine key phrases to form summary\n        summary = \" \".join(key_phrases)\n        summaries.append(summary)\n    return summaries","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:47:55.167601Z","iopub.execute_input":"2024-04-15T12:47:55.168408Z","iopub.status.idle":"2024-04-15T12:47:55.174957Z","shell.execute_reply.started":"2024-04-15T12:47:55.168376Z","shell.execute_reply":"2024-04-15T12:47:55.174120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply dependency parsing-based summarization to batches of rows in df2['chunks'] and store in df7\nbatch_size = 32\ndf3 = pd.DataFrame()\nfor i in range(0, len(df2), batch_size):\n    batch_texts = df2['chunks'].iloc[i:i+batch_size].tolist()\n    batch_summaries = generate_dependency_summary_batch(batch_texts)\n    df3 = pd.concat([df3, pd.DataFrame({'dependency_summary': batch_summaries})], ignore_index=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:48:31.690737Z","iopub.execute_input":"2024-04-15T12:48:31.691116Z","iopub.status.idle":"2024-04-15T12:48:37.868807Z","shell.execute_reply.started":"2024-04-15T12:48:31.691084Z","shell.execute_reply":"2024-04-15T12:48:37.868020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Original Text: \")\nprint(df2[\"chunks\"].iloc[0])\nprint(\"\\n\\n\")\nprint(\"Summary Generated via dependecy parsing: \")\nprint(df3[\"dependency_summary\"].iloc[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:49:05.686459Z","iopub.execute_input":"2024-04-15T12:49:05.686847Z","iopub.status.idle":"2024-04-15T12:49:05.693338Z","shell.execute_reply.started":"2024-04-15T12:49:05.686816Z","shell.execute_reply":"2024-04-15T12:49:05.692254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Comparing Parsing output to the Original summary in df frame**","metadata":{}},{"cell_type":"code","source":"# Compute ROUGE scores\nfrom rouge import Rouge\nrouge = Rouge()\nrouge_scores = rouge.get_scores(df3['dependency_summary'], df['summary'], avg=True)\n\n\nprint(\"ROUGE Scores:\")\nfor metric, score in rouge_scores.items():\n    print(f\"{metric}: {score}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:49:38.376644Z","iopub.execute_input":"2024-04-15T12:49:38.377015Z","iopub.status.idle":"2024-04-15T12:49:40.180713Z","shell.execute_reply.started":"2024-04-15T12:49:38.376985Z","shell.execute_reply":"2024-04-15T12:49:40.179685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute BERTScore scores\nfrom bert_score import score\n\n# Convert summaries to strings\nreference_summaries = [' '.join(summary.split()) for summary in df['summary']]\ngenerated_summaries = [' '.join(summary.split()) for summary in df3['dependency_summary']]\n\nP, R, F1 = score(generated_summaries, reference_summaries, lang='en', verbose=True)\nprint(\"BERTScore Precision:\", P.mean().item())\nprint(\"BERTScore Recall:\", R.mean().item())\nprint(\"BERTScore F1:\", F1.mean().item())","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:49:59.891930Z","iopub.execute_input":"2024-04-15T12:49:59.892888Z","iopub.status.idle":"2024-04-15T12:50:21.152949Z","shell.execute_reply.started":"2024-04-15T12:49:59.892852Z","shell.execute_reply":"2024-04-15T12:50:21.152017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **BART Model With Output Given by Dependency Parsing**","metadata":{}},{"cell_type":"markdown","source":"1. 'dependency_summary' column of df3 frame contains the chunks after applying Dependency Parsing on it.\n2. We Experimented with different summarization models from Hugging Face; BART emerged as the top performer with the highest Rouge score (Rouge-1 f=0.54).\n3. Now, our focus shifts to evaluating whether incorporating dependency parsing data, specifically df3['dependency_summary'], enhances the performance of the BART model.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers torch\n!pip install rouge\n!pip install bert_score\n\nimport torch\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T13:08:15.879782Z","iopub.execute_input":"2024-04-15T13:08:15.880518Z","iopub.status.idle":"2024-04-15T13:30:43.049561Z","shell.execute_reply.started":"2024-04-15T13:08:15.880486Z","shell.execute_reply":"2024-04-15T13:30:43.048293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T13:33:08.867008Z","iopub.execute_input":"2024-04-15T13:33:08.867468Z","iopub.status.idle":"2024-04-15T13:33:08.873126Z","shell.execute_reply.started":"2024-04-15T13:33:08.867436Z","shell.execute_reply":"2024-04-15T13:33:08.872092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, BartTokenizer\n\ndef generate_bart_summary_batch(texts):\n    # Load tokenizer\n    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n    # Load model\n    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n    model.to(device)  # Move model to GPU\n    summaries = []\n    for text in texts:\n        # Tokenize the text\n        inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"longest\")\n        inputs.to(device)  # Move input tensors to GPU\n\n        # Generate summary\n        summary_ids = model.generate(inputs.input_ids.to(device), max_length=100, num_beams=5, early_stopping=True, min_length=30)\n\n        # Decode and append the summary\n        summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n\n    return summaries\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T13:33:14.756406Z","iopub.execute_input":"2024-04-15T13:33:14.756796Z","iopub.status.idle":"2024-04-15T13:33:14.764165Z","shell.execute_reply.started":"2024-04-15T13:33:14.756758Z","shell.execute_reply":"2024-04-15T13:33:14.763079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply BART summarization to batches of rows in df2['chunks'] and store in df7\nbatch_size = 32\ndf4 = pd.DataFrame()\nfor i in range(0, len(df2), batch_size):\n    batch_texts = df3['dependency_summary'].iloc[i:i+batch_size].tolist()\n    batch_summaries = generate_bart_summary_batch(batch_texts)\n    df4 = pd.concat([df4, pd.DataFrame({'bart_summary': batch_summaries})], ignore_index=True)\n\ndf4.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T13:33:58.262989Z","iopub.execute_input":"2024-04-15T13:33:58.263943Z","iopub.status.idle":"2024-04-15T13:35:56.124797Z","shell.execute_reply.started":"2024-04-15T13:33:58.263910Z","shell.execute_reply":"2024-04-15T13:35:56.124028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute ROUGE scores\nfrom rouge import Rouge\n\nrouge = Rouge()\nrouge_scores = rouge.get_scores(df4['bart_summary'], df['summary'], avg=True)\n\n\nprint(\"ROUGE Scores:\")\nfor metric, score in rouge_scores.items():\n    print(f\"{metric}: {score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T13:36:09.350054Z","iopub.execute_input":"2024-04-15T13:36:09.350449Z","iopub.status.idle":"2024-04-15T13:36:09.860004Z","shell.execute_reply.started":"2024-04-15T13:36:09.350417Z","shell.execute_reply":"2024-04-15T13:36:09.858978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute BERTScore scores\nfrom bert_score import score\n\n# Convert summaries to strings\nreference_summaries = [' '.join(summary.split()) for summary in df['summary']]\ngenerated_summaries = [' '.join(summary.split()) for summary in df4['bart_summary']]\n\nP, R, F1 = score(generated_summaries, reference_summaries, lang='en', verbose=True)\nprint(\"BERTScore Precision:\", P.mean().item())\nprint(\"BERTScore Recall:\", R.mean().item())\nprint(\"BERTScore F1:\", F1.mean().item())","metadata":{"execution":{"iopub.status.busy":"2024-04-15T13:37:58.593374Z","iopub.execute_input":"2024-04-15T13:37:58.594145Z","iopub.status.idle":"2024-04-15T13:38:10.081404Z","shell.execute_reply.started":"2024-04-15T13:37:58.594114Z","shell.execute_reply":"2024-04-15T13:38:10.080503Z"},"trusted":true},"execution_count":null,"outputs":[]}]}